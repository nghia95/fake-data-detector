{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Optimisation\n",
    "import swifter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, cmudict, stopwords\n",
    "from collections import Counter\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"averaged_perceptron_tagger\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"cmudict\")\n",
    "\n",
    "# Topic modelling\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Readability & sentiment analysis\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "\n",
    "# ML\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from a CSV file.\"\"\"\n",
    "    data = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    df = data.copy()\n",
    "    df = df.drop(columns=[\"prompt_id\", \"text_length\", \"word_count\"])\n",
    "    df[\"source\"] = df[\"source\"].apply(lambda x: 1 if x != \"Human\" else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        self.symbols_to_keep = {\"$\", \"-\", \"%\"}\n",
    "\n",
    "    def get_wordnet_pos(slef,text):\n",
    "        \"\"\"Convert NLTK POS tag to a format suitable for WordNet Lemmatizer.\"\"\"\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        tag = nltk.pos_tag([text])[0][1][0].upper() if text else \"N\" # Default to NOUN\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def text_cleaning(self,text):\n",
    "        \"\"\"Clean text but preserve some symbols and numbers.\"\"\"\n",
    "        if not isinstance(text, str):  # Handle non-string input\n",
    "            return \"\"\n",
    "\n",
    "        text = text.lower().strip() # Turn to all lowercase & remove whitespace\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text) # Remove non-ASCII characters\n",
    "        text_clean = \"\".join(char if char not in string.punctuation or char in self.symbols_to_keep else \"\" for char in text)\n",
    "\n",
    "        tokenized = word_tokenize(text_clean)\n",
    "        tagged_words = nltk.pos_tag(tokenized)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(word)) for word, _ in tagged_words]\n",
    "        cleaned_text = \" \".join(lemmatized)\n",
    "        return cleaned_text\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply cleaning to text column.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame) and \"text\" in X.columns:\n",
    "            X[\"text\"] = X[\"text\"].apply(self.text_cleaning)\n",
    "            return X\n",
    "\n",
    "        elif isinstance(X, pd.Series):\n",
    "            return X.apply(self.text_cleaning)\n",
    "\n",
    "        raise ValueError(\"Expected DataFrame or Series with a 'text' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.cmu_dict = cmudict.dict()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.common_ai_words = set([\n",
    "            \"commendable\", \"transhumanist\", \"meticulous\", \"elevate\", \"hello\", \"tapestry\", \"leverage\",\n",
    "            \"journey\", \"headache\", \"resonate\", \"testament\", \"explore\", \"binary\", \"delve\",\n",
    "            \"enrich\", \"seamless\", \"multifaceted\", \"sorry\", \"foster\", \"convey\", \"beacon\",\n",
    "            \"interplay\", \"oh\", \"navigate\", \"form\", \"adhere\", \"cannot\", \"landscape\", \"remember\",\n",
    "            \"paramount\", \"comprehensive\", \"placeholder\", \"grammar\", \"real\", \"summary\", \"symphony\",\n",
    "            \"furthermore\", \"relationship\", \"ultimately\", \"profound\", \"art\", \"supercharge\", \"evolve\",\n",
    "            \"beyond\", \"reimagine\", \"vibrant\", \"robust\", \"pivotal\", \"certainly\", \"quinoa\", \"orchestrate\", \"align\",\n",
    "            \"diverse\", \"recommend\", \"annals\", \"note\", \"employ\", \"bustling\", \"indeed\", \"digital\", \"enigma\", \"outfit\",\n",
    "            \"indelible\", \"refrain\", \"culture\", \"treat\", \"emerge\", \"esteemed\", \"weight\", \"whimsical\", \"bespoke\",\n",
    "            \"highlight\", \"antagonist\", \"unlock\", \"key\", \"breakdown\", \"tailor\", \"misinformation\", \"treasure\",\n",
    "            \"paradigm\", \"captivate\", \"song\", \"underscore\", \"calculate\", \"especially\", \"climate\", \"hedging\",\n",
    "            \"inclusive\", \"exercise\", \"ai\", \"embrace\", \"level\", \"nuance\", \"career\", \"dynamic\", \"accent\",\n",
    "            \"ethos\", \"cheap\", \"firstly\", \"online\", \"goodbye\"])\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    def get_word_stress(self, word):\n",
    "        \"\"\"Calculate stress score for a single word using CMU dictionary.\"\"\"\n",
    "        word = word.lower()\n",
    "\n",
    "        if word in self.cmu_dict:\n",
    "            return sum(int(char) for syllable in self.cmu_dict[word][0] for char in syllable if char.isdigit())\n",
    "\n",
    "        elif word.upper() in self.cmu_dict:\n",
    "            return sum(int(char) for syllable in self.cmu_dict[word.upper()][0] for char in syllable if char.isdigit())\n",
    "\n",
    "        return 0  # If not found, return 0\n",
    "\n",
    "\n",
    "    def get_sentence_stress(self, sentence):\n",
    "        \"\"\"Calculate total stress score for a sentence.\"\"\"\n",
    "        if not isinstance(sentence, str) or not sentence.strip():\n",
    "            return 0\n",
    "\n",
    "        words = sentence.split()\n",
    "        stress_values = [self.get_word_stress(word) for word in words]\n",
    "        return sum(stress_values)\n",
    "\n",
    "\n",
    "    def cons_density(self, text):\n",
    "        \"\"\"Calculate consonant density in text.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0\n",
    "        consonant = sum(1 for char in text if char.isalpha() and char not in \"aeiouAEIOU\")\n",
    "        vowel = sum(1 for char in text if char.isalpha() and char in \"aeiouAEIOU\")\n",
    "        return round((consonant/(vowel + consonant)),3)\n",
    "\n",
    "\n",
    "    def redundance(self, text):\n",
    "        \"\"\"Compute redundancy score based on repeated words.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        clean_tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        final_lemmas = [self.lemmatizer.lemmatize(self.lemmatizer.lemmatize(word, 'v'), 'n') for word in clean_tokens]\n",
    "\n",
    "        word_counts = Counter(final_lemmas)\n",
    "        if len(word_counts) == 0:\n",
    "            return 0  # Prevent division by zero if no tokens remain\n",
    "\n",
    "        mean_freq = sum(word_counts.values()) / len(word_counts)\n",
    "        score = sum(1 for word, count in word_counts.items() if count > 3 * mean_freq)\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "    def sentiment_polarity(self, text):\n",
    "        \"\"\"Compute sentiment polarity (0 to 1), with smoothing for better balance.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0  # Handle empty or None inputs safely\n",
    "\n",
    "        sent_pol = TextBlob(text).sentiment.polarity\n",
    "        abs_pol = abs(round(sent_pol, 3))\n",
    "        return 0.0 if abs_pol < 0.1 else min(abs_pol, 0.8)\n",
    "\n",
    "\n",
    "    def word_choice(self, text):\n",
    "        \"\"\"Count number of AI-associated words in the text.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0\n",
    "        return sum(1 for word in text.split() if word in self.common_ai_words)\n",
    "\n",
    "\n",
    "    def coherence(self, text):\n",
    "        \"\"\"Compute coherence score using LSA model.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        if not tokens:\n",
    "            coherence_score = 0\n",
    "        else:\n",
    "            dictionary = corpora.Dictionary([tokens])\n",
    "            corpus_gensim = [dictionary.doc2bow(tokens)]\n",
    "            lsa_model = LsiModel(corpus_gensim, id2word=dictionary, num_topics=5)\n",
    "\n",
    "            coherence_model = CoherenceModel(\n",
    "                model=lsa_model,\n",
    "                texts=[tokens],\n",
    "                dictionary=dictionary,\n",
    "                coherence='c_v')\n",
    "\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "        return coherence_score\n",
    "\n",
    "\n",
    "    def reading_ease(self, text):\n",
    "        \"\"\"Returns Flesch Reading Ease score (higher = easier to read).\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0  # Handle empty or None input safely\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "\n",
    "\n",
    "    def gunning_fog(self,text):\n",
    "        \"\"\"Returns Gunning Fog Index (higher = more difficult to read).\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0  # Handle empty or None input safely\n",
    "        return textstat.gunning_fog(text)\n",
    "\n",
    "\n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Process a batch of text inputs using Swifter for faster feature extraction.\"\"\"\n",
    "        if isinstance(text, list) or isinstance(text, np.ndarray):\n",
    "            text = pd.Series(text)  # Ensure Swifter works properly\n",
    "\n",
    "        results = text.swifter.apply(lambda t: [\n",
    "            np.mean([self.get_word_stress(word) for word in t.split()]) if t.split() else 0,\n",
    "            self.get_sentence_stress(t),\n",
    "            self.cons_density(t),\n",
    "            self.redundance(t),\n",
    "            self.sentiment_polarity(t),\n",
    "            self.word_choice(t),\n",
    "            self.coherence(t),\n",
    "            self.reading_ease(t),\n",
    "            self.gunning_fog(t)\n",
    "        ])\n",
    "\n",
    "        return np.array(results.tolist())\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the scaler using training data.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame) and \"text\" in X.columns:\n",
    "            features = self.extract_features(X[\"text\"])\n",
    "        else:\n",
    "            features = self.extract_features(X)\n",
    "\n",
    "        self.scaler.fit(features)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Extract and scale features from text column.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame) and \"text\" in X.columns:\n",
    "            features = self.extract_features(X[\"text\"])\n",
    "        else:\n",
    "            features = self.extract_features(X)\n",
    "\n",
    "        return self.scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "class TFIDFVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features=10000, ngram_range=(1, 2), stop_words=\"english\"):\n",
    "        \"\"\"\n",
    "        Custom TF-IDF vectorizer with optimizations.\n",
    "        - max_features: Limits vocabulary size to the most important words.\n",
    "        - ngram_range: (1,1) for unigrams, (1,2) for bigrams, etc.\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        self.stop_words = stop_words\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words=stop_words)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame) and \"text\" in X.columns:\n",
    "            X = X[\"text\"]  # Extract the 'text' column\n",
    "        if not isinstance(X, (list, pd.Series)) or len(X) == 0:\n",
    "            raise ValueError(\"Input must be a non-empty list or pandas Series of text.\")\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transforms text data into TF-IDF feature vectors.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame) and \"text\" in X.columns:\n",
    "            X = X[\"text\"]  # Extract the 'text' column\n",
    "        if not isinstance(X, (list, pd.Series)) or len(X) == 0:\n",
    "            raise ValueError(\"Input must be a non-empty list or pandas Series of text.\")\n",
    "        return self.vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer:\n",
    "    \"\"\"Ensures output is converted to a dense NumPy array.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Traing & Evaluation\n",
    "\n",
    "class GradientBoostingClassifierWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.1, max_depth=3, early_stopping_rounds=10):\n",
    "        \"\"\"\n",
    "        Custom wrapper for Gradient Boosting Classifier.\n",
    "        - n_estimators: Number of boosting stages.\n",
    "        - learning_rate: Shrinks contribution of each tree.\n",
    "        - max_depth: Maximum depth of individual estimators.\n",
    "        - early_stopping_rounds: Stops training when validation loss doesn't improve.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train Gradient Boosting model.\"\"\"\n",
    "        if X is None or y is None or len(X) == 0 or len(y) == 0:\n",
    "            raise ValueError(\"Training data (X, y) must not be empty.\")\n",
    "\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators = self.n_estimators,\n",
    "            learning_rate = self.learning_rate,\n",
    "            max_depth = self.max_depth,\n",
    "            n_iter_no_change = self.early_stopping_rounds)\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict with the trained Gradient Boosting model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call `fit()` first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call `fit()` first.\")\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "class EvaluationPipeline:\n",
    "    def __init__(self, model):\n",
    "        \"\"\"Initialize with a trained model.\"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit method (not used, but required for Scikit-learn compatibility).\"\"\"\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate the trained model on test data.\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        results = {\n",
    "            \"loss\": log_loss(y_test, y_pred_proba),\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_pred_proba)}\n",
    "\n",
    "        # Print performance\n",
    "        print(\"Model Performance:\")\n",
    "        print(f\"Loss: {results['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {results['precision']:.4f}\")\n",
    "        print(f\"Recall: {results['recall']:.4f}\")\n",
    "        print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "        # Compare Actual vs. Predicted\n",
    "        comparison_df = pd.DataFrame({\n",
    "            \"Actual\": y_test.values,\n",
    "            \"Predicted\": y_pred,\n",
    "            \"Probability\": y_pred_proba})\n",
    "\n",
    "        print(\"**Actual vs. Predicted Results:**\")\n",
    "        print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning_pipeline = Pipeline([('clean_text', TextCleaner())])\n",
    "\n",
    "feature_extraction_pipeline = Pipeline([(\"feature_extraction\", FeatureExtractor())])\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    (\"tfidf\", TFIDFVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\"))])\n",
    "\n",
    "# Combine TF-IDF with the Scaled Feature Extraction\n",
    "combined_features = Pipeline([\n",
    "    (\"feature_union\", FeatureUnion([\n",
    "        (\"tfidf\", tfidf_pipeline),\n",
    "        (\"scaled_features\", feature_extraction_pipeline)])),\n",
    "    (\"to_dense\", DenseTransformer())]) # Convert to NumPy array\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    (\"text_cleaning\", text_cleaning_pipeline),\n",
    "    (\"combined_features\", combined_features),\n",
    "    (\"classifier\", GradientBoostingClassifierWrapper(n_estimators=200, learning_rate=0.1, max_depth=3))])\n",
    "\n",
    "evaluator = EvaluationPipeline(model=final_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>The Jewish State: The Struggle for Israel’s So...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>turns on her side and then lays down on the fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>Alphabetical order is not the optimal layout, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>We propose a two-layer cache mechanism to spee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Jane Austen's novels have stood the test of ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  source\n",
       "521  The Jewish State: The Struggle for Israel’s So...       0\n",
       "737  turns on her side and then lays down on the fl...       1\n",
       "740  Alphabetical order is not the optimal layout, ...       1\n",
       "660  We propose a two-layer cache mechanism to spee...       1\n",
       "411  Jane Austen's novels have stood the test of ti...       1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(\"../data/1k_sampled_dataset.csv\")\n",
    "df_sample = df.sample(30, random_state=42)  # ✅ Take only 50 rows\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 24/24 [00:39<00:00,  1.63s/it]\n",
      "Pandas Apply: 100%|██████████| 24/24 [00:37<00:00,  1.56s/it]\n",
      "Pandas Apply: 100%|██████████| 6/6 [00:09<00:00,  1.54s/it]\n",
      "Pandas Apply: 100%|██████████| 6/6 [00:09<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Loss: 0.5942\n",
      "Accuracy: 0.8333\n",
      "Precision: 0.7500\n",
      "Recall: 1.0000\n",
      "ROC-AUC: 0.8333\n",
      "**Actual vs. Predicted Results:**\n",
      "   Actual  Predicted  Probability\n",
      "0       0          1     0.966063\n",
      "1       1          1     0.966063\n",
      "2       1          1     0.966063\n",
      "3       0          0     0.031003\n",
      "4       0          0     0.045595\n",
      "5       1          1     0.966063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_sample[\"text\"]  # Raw text input\n",
    "y = df_sample[\"source\"]  # Labels (0=Human, 1=AI)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "results = evaluator.evaluate(X_test, y_test)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-data-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
