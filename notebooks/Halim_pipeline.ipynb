{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, cmudict, stopwords\n",
    "from collections import Counter\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"averaged_perceptron_tagger\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"cmudict\")\n",
    "\n",
    "# Topic modelling\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Readability & sentiment analysis\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "\n",
    "# ML\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from a CSV file.\"\"\"\n",
    "    data = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    df = data.copy()\n",
    "    df[\"source\"] = df[\"source\"].apply(lambda x: \"AI\" if x != \"Human\" else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        self.symbols_to_keep = {\"$\", \"-\", \"%\"}\n",
    "\n",
    "    def get_wordnet_pos(slef,text):\n",
    "        \"\"\"Convert NLTK POS tag to a format suitable for WordNet Lemmatizer.\"\"\"\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        tag = nltk.pos_tag([text])[0][1][0].upper() if text else \"N\" # Default to NOUN\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def text_cleaning(self,text):\n",
    "        \"\"\"Clean text but preserve some symbols and numbers.\"\"\"\n",
    "        if not isinstance(text, str):  # Handle non-string input\n",
    "            return \"\"\n",
    "\n",
    "        text = text.lower().strip() # Turn to all lowercase & remove whitespace\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text) # Remove non-ASCII characters\n",
    "        text_clean = \"\".join(char if char not in string.punctuation or char in symbols_to_keep else \"\" for char in text)\n",
    "\n",
    "        tokenized = word_tokenize(text_clean)\n",
    "        tagged_words = nltk.pos_tag(tokenized)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(word)) for word, _ in tagged_words]\n",
    "        cleaned_text = \" \".join(lemmatized)\n",
    "        return cleaned_text\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply text cleaning transformation.\"\"\"\n",
    "        return X.apply(self.text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.cmu_dict = cmudict.dict()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.common_ai_words = set([\n",
    "            \"commendable\", \"transhumanist\", \"meticulous\", \"elevate\", \"hello\", \"tapestry\", \"leverage\",\n",
    "            \"journey\", \"headache\", \"resonate\", \"testament\", \"explore\", \"binary\", \"delve\",\n",
    "            \"enrich\", \"seamless\", \"multifaceted\", \"sorry\", \"foster\", \"convey\", \"beacon\",\n",
    "            \"interplay\", \"oh\", \"navigate\", \"form\", \"adhere\", \"cannot\", \"landscape\", \"remember\",\n",
    "            \"paramount\", \"comprehensive\", \"placeholder\", \"grammar\", \"real\", \"summary\", \"symphony\",\n",
    "            \"furthermore\", \"relationship\", \"ultimately\", \"profound\", \"art\", \"supercharge\", \"evolve\",\n",
    "            \"beyond\", \"reimagine\", \"vibrant\", \"robust\", \"pivotal\", \"certainly\", \"quinoa\", \"orchestrate\", \"align\",\n",
    "            \"diverse\", \"recommend\", \"annals\", \"note\", \"employ\", \"bustling\", \"indeed\", \"digital\", \"enigma\", \"outfit\",\n",
    "            \"indelible\", \"refrain\", \"culture\", \"treat\", \"emerge\", \"esteemed\", \"weight\", \"whimsical\", \"bespoke\",\n",
    "            \"highlight\", \"antagonist\", \"unlock\", \"key\", \"breakdown\", \"tailor\", \"misinformation\", \"treasure\",\n",
    "            \"paradigm\", \"captivate\", \"song\", \"underscore\", \"calculate\", \"especially\", \"climate\", \"hedging\",\n",
    "            \"inclusive\", \"exercise\", \"ai\", \"embrace\", \"level\", \"nuance\", \"career\", \"dynamic\", \"accent\",\n",
    "            \"ethos\", \"cheap\", \"firstly\", \"online\", \"goodbye\"])\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    def get_word_stress(self, word):\n",
    "        \"\"\"Calculate stress score for a single word using CMU dictionary.\"\"\"\n",
    "        if word in cmu_dict:\n",
    "            return sum(int(char) for syllable in cmu_dict[word][0] for char in syllable if char.isdigit())\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def get_sentence_stress(self, sentence):\n",
    "        \"\"\"Calculate total stress score for a sentence.\"\"\"\n",
    "        if not isinstance(sentence, str) or not sentence.strip():\n",
    "            return 0\n",
    "        words = sentence.split()\n",
    "        stress_values = [get_word_stress(word) for word in words]\n",
    "        return sum(stress_values)\n",
    "\n",
    "\n",
    "    def cons_density(self, text):\n",
    "        \"\"\"Calculate consonant density in text.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0\n",
    "        consonant = sum(1 for char in text if char.isalpha() and char not in \"aeiouAEIOU\")\n",
    "        vowel = sum(1 for char in text if char.isalpha() and char in \"aeiouAEIOU\")\n",
    "        return round((consonant/(vowel + consonant)),3)\n",
    "\n",
    "\n",
    "    def redundance(self, text):\n",
    "        \"\"\"Compute redundancy score based on repeated words.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        clean_tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        final_lemmas = [self.lemmatizer.lemmatize(self.lemmatizer.lemmatize(word, 'v'), 'n') for word in clean_tokens]\n",
    "\n",
    "        word_counts = Counter(final_lemmas)\n",
    "        if len(word_counts) == 0:\n",
    "            return 0  # Prevent division by zero if no tokens remain\n",
    "\n",
    "        mean_freq = sum(word_counts.values()) / len(word_counts)\n",
    "        score = sum(1 for word, count in word_counts.items() if count > 3 * mean_freq)\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "    def sentiment_polarity(self, text):\n",
    "        \"\"\"Compute sentiment polarity (0 to 1), with smoothing for better balance.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0  # Handle empty or None inputs safely\n",
    "\n",
    "        sent_pol = TextBlob(text).sentiment.polarity\n",
    "        abs_pol = abs(round(sent_pol, 3))\n",
    "        return 0.0 if abs_pol < 0.1 else min(abs_pol, 0.8)\n",
    "\n",
    "\n",
    "    def word_choice(self, text):\n",
    "        \"\"\"Count number of AI-associated words in the text.\"\"\"\n",
    "        return sum(1 for word in text.split() if word in self.common_ai_words)\n",
    "\n",
    "\n",
    "    def coherence(self, text):\n",
    "        \"\"\"Compute coherence score using LSA model.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        if not tokens:\n",
    "            coherence_score = 0\n",
    "        else:\n",
    "            dictionary = corpora.Dictionary([tokens])\n",
    "            corpus_gensim = [dictionary.doc2bow(tokens)]\n",
    "            lsa_model = LsiModel(corpus_gensim, id2word=dictionary, num_topics=5)\n",
    "\n",
    "            coherence_model = CoherenceModel(\n",
    "                model=lsa_model,\n",
    "                texts=[tokens],\n",
    "                dictionary=dictionary,\n",
    "                coherence='c_v')\n",
    "\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "        return coherence_score\n",
    "\n",
    "\n",
    "    def reading_ease(text):\n",
    "        \"\"\"Returns Flesch Reading Ease score (higher = easier to read).\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0  # Handle empty or None input safely\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "\n",
    "\n",
    "    def gunning_fog(text):\n",
    "        \"\"\"Returns Gunning Fog Index (higher = more difficult to read).\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0.0  # Handle empty or None input safely\n",
    "        return textstat.gunning_fog(text)\n",
    "\n",
    "\n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Process a batch of text inputs.\"\"\"\n",
    "        return np.array([\n",
    "            [self.get_word_stress(text),\n",
    "             self.get_sentence_stress(text),\n",
    "             self.cons_density(text),\n",
    "             self.redundance(text),\n",
    "             self.sentiment_polarity(text),\n",
    "             self.word_choice(text),\n",
    "             self.coherence(text),\n",
    "             self.reading_ease(text),\n",
    "             self.gunning_fog(text)]\n",
    "            for t in text])\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the scaler using training data.\"\"\"\n",
    "        features = self.extract_features(X)\n",
    "        self.scaler.fit(features)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Extract and scale features.\"\"\"\n",
    "        features = self.extract_features(X)\n",
    "        return self.scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "class TFIDFVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features=10000, ngram_range=(1, 2), stop_words=\"english\"):\n",
    "        \"\"\"\n",
    "        Custom TF-IDF vectorizer with optimizations.\n",
    "        - max_features: Limits vocabulary size to the most important words.\n",
    "        - ngram_range: (1,1) for unigrams, (1,2) for bigrams, etc.\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words=stop_words)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, (list, pd.Series)) or len(X) == 0:\n",
    "            raise ValueError(\"Input must be a non-empty list or pandas Series of text.\")\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transforms text data into TF-IDF feature vectors.\"\"\"\n",
    "        if not isinstance(X, (list, pd.Series)) or len(X) == 0:\n",
    "            raise ValueError(\"Input must be a non-empty list or pandas Series of text.\")\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_transformer = TFIDFVectorizer(max_features=5000, ngram_range=(1, 2), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training & Evaluation\n",
    "\n",
    "# Data Splitting\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Splits data into training and test sets.\"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "# Training\n",
    "class GradientBoostingClassifierWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.1, max_depth=3, early_stopping_rounds=10):\n",
    "        \"\"\"\n",
    "        Custom wrapper for Gradient Boosting Classifier.\n",
    "        - n_estimators: Number of boosting stages.\n",
    "        - learning_rate: Shrinks contribution of each tree.\n",
    "        - max_depth: Maximum depth of individual estimators.\n",
    "        - early_stopping_rounds: Stops training when validation loss doesn't improve.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train Gradient Boosting model.\"\"\"\n",
    "        if X is None or y is None or len(X) == 0 or len(y) == 0:\n",
    "            raise ValueError(\"Training data (X, y) must not be empty.\")\n",
    "\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators = self.n_estimators,\n",
    "            learning_rate = self.learning_rate,\n",
    "            max_depth = self.max_depth,\n",
    "            es = self.early_stopping_rounds)\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict with the trained Gradient Boosting model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call `fit()` first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call `fit()` first.\")\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "class EvaluationPipeline:\n",
    "    def __init__(self, model):\n",
    "        \"\"\"Initialize with a trained model.\"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit method (not used, but required for Scikit-learn compatibility).\"\"\"\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate the trained model on test data.\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        results = {\n",
    "            \"loss\": log_loss(y_test, y_pred_proba),\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        print(\"\\nðŸ“Š **Model Performance:**\")\n",
    "        print(f\"âŒ Loss: {results['loss']:.4f}\")\n",
    "        print(f\"âœ… Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {results['precision']:.4f}\")\n",
    "        print(f\"Recall: {results['recall']:.4f}\")\n",
    "        print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{results['confusion_matrix']}\\n\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# End-to-End Model Training & Evaluation\n",
    "def train_and_evaluate(X, y, n_estimators=200, learning_rate=0.1, max_depth=3, early_stopping_rounds=10):\n",
    "    \"\"\"\n",
    "    Splits data, trains the model, and evaluates performance.\n",
    "    Allows customization of Gradient Boosting hyperparameters.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "    # Train the model\n",
    "    model_params = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"early_stopping_rounds\": early_stopping_rounds\n",
    "    }\n",
    "    model = GradientBoostingClassifierWrapper(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = EvaluationPipeline(model=model)\n",
    "    results = evaluator.evaluate(X_test, y_test)\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning_pipeline = Pipeline([('clean_text', TextCleaner())])\n",
    "\n",
    "feature_extraction_pipeline = Pipeline([(\"feature_extraction\", FeatureExtractor())])\n",
    "\n",
    "tfidf_pipeline = Pipeline([(\"tfidf\", TFIDFVectorizer(max_features=10000))])\n",
    "\n",
    "# Combine TF-IDF with the Scaled Feature Extraction\n",
    "combined_features = FeatureUnion([\n",
    "    (\"tfidf\", tfidf_pipeline),\n",
    "    (\"scaled_features\", feature_extraction_pipeline)])\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    (\"text_cleaning\", text_cleaning_pipeline),\n",
    "    (\"combined_features\", combined_features),\n",
    "    (\"classifier\", GradientBoostingClassifierWrapper(n_estimators=200, learning_rate=0.1, max_depth=3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "# Train the model\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = final_pipeline.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-data-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
