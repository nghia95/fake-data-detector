{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(x):\n",
    "    # remove whitespace\n",
    "    x = x.strip()\n",
    "    # lowercasing\n",
    "    x = x.lower()\n",
    "    # remove digits\n",
    "    x = \"\".join(char for char in x if not char.isdigit())\n",
    "    # remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        x = x.replace(punctuation,\" \")\n",
    "    # remove regex\n",
    "    x = re.sub('<[^<]+?',\"\",x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_density(text):\n",
    "\n",
    "    consonnant = sum(1 for char in text if char.isalpha() and char not in \"aeiou\")\n",
    "    vowel = sum(1 for char in text if char.isalpha() and char in \"aeiou\")\n",
    "    return round((consonnant/(vowel + consonnant)),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_stress(word):\n",
    "    if word in cmu_dict:\n",
    "        return sum(int(char) for syllable in cmu_dict[word][0] for char in syllable if char.isdigit())\n",
    "    return 0\n",
    "\n",
    "def get_sentence_stress(sentence):\n",
    "    words = sentence.split()\n",
    "    stress_values = [get_word_stress(word) for word in words]\n",
    "    return sum(stress_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundance(text):\n",
    "    # give a redundance score, considering the lenght of each text, if a lemmatized words appears more than three times the mean, it is considered redundant.\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    verb_lemmas = {word: lemmatizer.lemmatize(word, pos='v') for word in clean_tokens}\n",
    "\n",
    "    final_lemmas = [lemmatizer.lemmatize(lemma, pos='n') if lemma == word else lemma\n",
    "                    for word, lemma in verb_lemmas.items()]\n",
    "\n",
    "    word_counts = Counter(final_lemmas)\n",
    "    mean_freq = sum(word_counts.values()) / len(word_counts)\n",
    "\n",
    "    score = sum(1 for word, count in word_counts.items() if count > 3 * mean_freq)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_polarity(text):\n",
    "    sent_pol = TextBlob(text).sentiment.polarity\n",
    "    return abs(round(sent_pol,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_choice(text):\n",
    "    common_ai_words =[\"commendable\",'transhumanist', 'meticulous', 'elevate','hello', 'tapestry' 'leverage',\n",
    "                  'journey', 'headache','resonate','testament','explore', 'binary','delve',\n",
    "                  'enrich', 'seamless','multifaceted', 'sorry','foster', 'convey', 'beacon',\n",
    "                  'interplay', 'oh', 'navigate','form','adhere','cannot', 'landscape','remember',\n",
    "                  'paramount', 'comprehensive', 'placeholder','grammar','real','summary','symphony',\n",
    "                  'furthermore','relationship','ultimately','profound','art','supercharge','evolve',\n",
    "                  'beyoud','reimagine','vibrant', 'robust','pivotal','certainly','quinoa','orchestrate','align',\n",
    "                  'diverse','recommend','annals','note','employ','bustling','indeed','digital','enigma', 'outfit',\n",
    "                  'indelible','refrain','culture','treat','emerge','meticulous','esteemed','weight','whimsical','bespoke',\n",
    "                  'highlight','antagonist','unlock','key','breakdown','tailor','misinformation','treasure','paradigm','captivate',\n",
    "                  'song','underscore','calculate','especially','climate','hedging','inclusive','exercise','ai','embrace',\n",
    "                  'level','nuance','career','dynamic','accent','ethos','cheap','firstly','online','goodbye'\n",
    "                  ]\n",
    "    word_count = 0\n",
    "    for word in text.split():\n",
    "        if word in common_ai_words:\n",
    "            word_count += 1\n",
    "        else: pass\n",
    "\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(text):\n",
    "    # uses gensim to measure coherence, use the lsi model(latent semantic indexing, coherence c_v because we provide the text)\n",
    "    tokens = word_tokenize(text)\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    corpus_gensim = [dictionary.doc2bow(tokens)]\n",
    "    lsa_model = LsiModel(corpus_gensim, id2word=dictionary)\n",
    "\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lsa_model,\n",
    "        texts=[tokens],\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_ease(text):\n",
    "    reading_ease= textstat.flesch_reading_ease(text)\n",
    "    return reading_ease\n",
    "\n",
    "\n",
    "def gunning_fog(text):\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    return gunning_fog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline\n",
    "We want to add columns, not transform them ==> no ColumnTransformer <br>\n",
    "Function transformer?<br>\n",
    "But firt we need to get our preprocessed data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Philosophy and Ethics of Transhumanism\\n\\n...</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1920</td>\n",
       "      <td>2558</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime-tracking app Citizen is launching its ow...</td>\n",
       "      <td>Flan-T5-XXL</td>\n",
       "      <td>0</td>\n",
       "      <td>378</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The court in Novorossiysk gave two of the danc...</td>\n",
       "      <td>GLM-130B</td>\n",
       "      <td>0</td>\n",
       "      <td>621</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>then drops the drumsticks, poses, then walks o...</td>\n",
       "      <td>GPT-J</td>\n",
       "      <td>0</td>\n",
       "      <td>513</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On tally went to the beach. She found a sand d...</td>\n",
       "      <td>GPT-J</td>\n",
       "      <td>0</td>\n",
       "      <td>4984</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       source  prompt_id  \\\n",
       "0  The Philosophy and Ethics of Transhumanism\\n\\n...      GPT-3.5       1920   \n",
       "1  Crime-tracking app Citizen is launching its ow...  Flan-T5-XXL          0   \n",
       "2  The court in Novorossiysk gave two of the danc...     GLM-130B          0   \n",
       "3  then drops the drumsticks, poses, then walks o...        GPT-J          0   \n",
       "4  On tally went to the beach. She found a sand d...        GPT-J          0   \n",
       "\n",
       "   text_length  word_count  \n",
       "0         2558         394  \n",
       "1          378          62  \n",
       "2          621         109  \n",
       "3          513          90  \n",
       "4         4984         846  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_load = pd.read_csv(\"/home/romaric/code/nghia95/fake-data-detector/data/1k_sampled_dataset.csv\")\n",
    "data = data_load.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"AI_gen\"] = data[\"source\"].apply(lambda x: 0 if x == \"Human\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(data[\"text\"])\n",
    "y=data[\"AI_gen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"Applies basic cleaning to text.\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame({\"preprocessed\": X.apply(basic_cleaning)})\n",
    "\n",
    "class ConsDensity(BaseEstimator, TransformerMixin):\n",
    "    \"Extracts consonant density from preprocessed text.\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame({\"cons_density\": X[\"preprocessed\"].apply(cons_density)})\n",
    "\n",
    "class Stress(BaseEstimator, TransformerMixin):\n",
    "    \"Extracts sentence stress values.\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame({\"stress_value\": X[\"preprocessed\"].apply(get_sentence_stress)})\n",
    "\n",
    "class Sentiment(BaseEstimator, TransformerMixin):\n",
    "    \"Extracts sentiment score.\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame({\"sentiment_score\": X[\"preprocessed\"].apply(sentiment_polarity)})\n",
    "\n",
    "class Redundance(BaseEstimator, TransformerMixin):\n",
    "    \"Extracts redundancy score from text.\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame({\"redundance\": X[\"preprocessed\"].apply(redundance)})\n",
    "\n",
    "class UnusualWord(BaseEstimator, TransformerMixin):\n",
    "    \"Extract the number of unusual word from text\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return pd.DataFrame({\"unusual_words_count\": X[\"preprocessed\"].apply(word_choice)})\n",
    "\n",
    "class Coherence(BaseEstimator, TransformerMixin):\n",
    "    \"Extract the number of unusual word from text\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return pd.DataFrame({\"coherence_score\": X[\"preprocessed\"].apply(coherence)})\n",
    "\n",
    "class ReadingEase(BaseEstimator, TransformerMixin):\n",
    "    \"Extract the number of unusual word from text\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return pd.DataFrame({\"reading_ease\": X[\"text\"].apply(reading_ease)})\n",
    "\n",
    "class GunningFog(BaseEstimator, TransformerMixin):\n",
    "    \"Extract the number of unusual word from text\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return pd.DataFrame({\"gunningfog\": X[\"text\"].apply(gunning_fog)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "word_choice() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, TextPreprocessor()),  \n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, FeatureUnion([\n\u001b[1;32m      4\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcons_density\u001b[39m\u001b[38;5;124m\"\u001b[39m, ConsDensity()),\n\u001b[1;32m      5\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstress_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, Stress()),\n\u001b[1;32m      6\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, Sentiment()),\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredundance\u001b[39m\u001b[38;5;124m\"\u001b[39m, Redundance()),\n\u001b[0;32m----> 8\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnusualWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mword_choice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m      9\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherence\u001b[39m\u001b[38;5;124m\"\u001b[39m, coherence()),\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadingEase\u001b[39m\u001b[38;5;124m\"\u001b[39m, reading_ease()),\n\u001b[1;32m     11\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGunningFog\u001b[39m\u001b[38;5;124m\"\u001b[39m, gunning_fog()),\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     ]))\n\u001b[1;32m     14\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: word_choice() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", TextPreprocessor()),\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"cons_density\", ConsDensity()),\n",
    "        (\"stress_value\", Stress()),\n",
    "        (\"sentiment_score\", Sentiment()),\n",
    "        (\"redundance\", Redundance()),\n",
    "        (\"UnusualWord\",word_choice()),\n",
    "        (\"Coherence\", coherence()),\n",
    "        (\"ReadingEase\", reading_ease()),\n",
    "        (\"GunningFog\", gunning_fog()),\n",
    "\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Run the Pipeline ###\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "# Merge results with the original DataFrame\n",
    "X = pd.concat([X, X_transformed], axis=1)\n",
    "\n",
    "print(X.head())  # Preview results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-data-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
