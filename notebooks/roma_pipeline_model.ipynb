{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append(\"/home/romaric/code/nghia95/fake-data-detector/notebooks/roma_NTLK_Data_Cache\")\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmu_dict = cmudict.dict()  # This should load from the cache\n",
    "print(cmu_dict[\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    if not isinstance(text, str):  # Convert to string if it's not\n",
    "       text = str(text)\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(text):\n",
    "    if not isinstance(text, str):  # Convert to string if it's not\n",
    "       text = str(text)\n",
    "    # Remove whitespace\n",
    "    prepoc_text = text.strip()\n",
    "    # Lowercasing\n",
    "    prepoc_text = prepoc_text.lower()\n",
    "    # remove digits\n",
    "    prepoc_text = \"\".join(char for char in prepoc_text if not char.isdigit())\n",
    "    # remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        prepoc_text = prepoc_text.replace(punctuation,\" \")\n",
    "    # remove regex\n",
    "    prepoc_text = re.sub('<[^<]+?',\"\",prepoc_text)\n",
    "\n",
    "    return prepoc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_density(text):\n",
    "\n",
    "    consonnant = sum(1 for char in text if char.isalpha() and char not in \"aeiou\")\n",
    "    vowel = sum(1 for char in text if char.isalpha() and char in \"aeiou\")\n",
    "    total_letters = vowel + consonnant\n",
    "    return round((consonnant/(vowel + consonnant)),3) if total_letters > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "def get_word_stress(word):\n",
    "    if word in cmu_dict:\n",
    "        return sum(int(char) for syllable in cmu_dict[word][0] for char in syllable if char.isdigit())\n",
    "    return 0\n",
    "\n",
    "def get_sentence_stress(sentence):\n",
    "    words = sentence.split()\n",
    "    stress_values = [get_word_stress(word) for word in words]\n",
    "    return sum(stress_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundance(text):\n",
    "    # give a redundance score, considering the lenght of each text, if a lemmatized words appears more than three times the mean, it is considered redundant.\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in clean_tokens]\n",
    "\n",
    "    word_counts = Counter(lemmatized_tokens)\n",
    "    mean_freq = sum(word_counts.values()) / len(word_counts) if len(word_counts)!= 0 else 0\n",
    "\n",
    "    if mean_freq != 0:\n",
    "        score = sum(1 for word, count in word_counts.items() if count > 2.5 * mean_freq)\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_polarity(text):\n",
    "    sent_pol = TextBlob(text).sentiment.polarity\n",
    "    return abs(round(sent_pol,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_choice(text):\n",
    "    common_ai_words =[\"commendable\",'transhumanist', 'meticulous', 'elevate','hello', 'tapestry','leverage',\n",
    "                  'journey', 'headache','resonate','testament','explore', 'binary','delve',\n",
    "                  'enrich', 'seamless','multifaceted', 'sorry','foster', 'convey', 'beacon',\n",
    "                  'interplay', 'oh', 'navigate','form','adhere','cannot', 'landscape','remember',\n",
    "                  'paramount', 'comprehensive', 'placeholder','grammar','real','summary','symphony',\n",
    "                  'furthermore','relationship','ultimately','profound','art','supercharge','evolve',\n",
    "                  'beyoud','reimagine','vibrant', 'robust','pivotal','certainly','quinoa','orchestrate','align',\n",
    "                  'diverse','recommend','annals','note','employ','bustling','indeed','digital','enigma', 'outfit',\n",
    "                  'indelible','refrain','culture','treat','emerge','meticulous','esteemed','weight','whimsical','bespoke',\n",
    "                  'highlight','antagonist','unlock','key','breakdown','tailor','misinformation','treasure','paradigm','captivate',\n",
    "                  'song','underscore','calculate','especially','climate','hedging','inclusive','exercise','ai','embrace',\n",
    "                  'level','nuance','career','dynamic','accent','ethos','cheap','firstly','online','goodbye'\n",
    "                  ]\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    word_count = 0\n",
    "    for word in text.split():\n",
    "        if word in common_ai_words:\n",
    "            word_count += 1\n",
    "\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(text):\n",
    "    # uses gensim to measure coherence, use the lsi model(latent semantic indexing, coherence c_v because we provide the text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        coherence_score = 0\n",
    "    else:\n",
    "        dictionary = corpora.Dictionary([tokens])\n",
    "        corpus_gensim = [dictionary.doc2bow(tokens)]\n",
    "        lsa_model = LsiModel(corpus_gensim, id2word=dictionary)\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=lsa_model,\n",
    "            texts=[tokens],\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_ease(text):\n",
    "    reading_ease= textstat.flesch_reading_ease(text)\n",
    "    return reading_ease\n",
    "\n",
    "\n",
    "def gunning_fog(text):\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    return gunning_fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello is the cat cat cat name, it is commendable. The cat eat the cat fish that was in the bowl of the cat, the cat is a bad cat!\"\n",
    "print(f'word count :{word_count(text)}')\n",
    "print(f'cleaned :{basic_cleaning(text)}')\n",
    "print(f'consonnance density :{cons_density(text)}')\n",
    "print(f'stress value :{get_sentence_stress(text)}')\n",
    "print(f'redundance :{redundance(text)}')\n",
    "print(f'sentiment :{sentiment_polarity(text)}')\n",
    "print(f'unusual word count :{word_choice(text)}')\n",
    "print(f'coherence :{coherence(text)}')\n",
    "print(f'reading ease :{reading_ease(text)}')\n",
    "print(f'gunning fog :{gunning_fog(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline\n",
    "We want to add columns, not transform them ==> no ColumnTransformer <br>\n",
    "Function transformer?<br>\n",
    "But firt we need to get our preprocessed data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load = pd.read_csv(\"/home/romaric/code/nghia95/fake-data-detector/data/1k_sampled_dataset.csv\")\n",
    "data = data_load.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"AI_gen\"] = data[\"source\"].apply(lambda x: 0 if x == \"Human\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(data[\"text\"])\n",
    "y=data[\"AI_gen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test No parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputHandler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, str):\n",
    "            X = [X]\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame({\"text\": X})\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            if \"text\" not in X.columns:\n",
    "                raise ValueError(\"Input DataFrame must have a 'text' column\")\n",
    "        else:\n",
    "            X = pd.DataFrame({\"text\": list(X)})\n",
    "        return X\n",
    "\n",
    "class HowManyWords(BaseEstimator, TransformerMixin):\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"word_count\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X[\"text\"]\n",
    "        word_c = X.apply(word_count)\n",
    "        return pd.DataFrame({\"word_count\": word_c})\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"preprocessed\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X[\"text\"]\n",
    "        cleaned = X.apply(basic_cleaning)\n",
    "        return pd.DataFrame({\"preprocessed\": cleaned})\n",
    "\n",
    "class ConsDensity(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"cons_density\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"preprocessed\"].apply(cons_density).values.reshape(-1, 1)\n",
    "\n",
    "class Stress(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"stress_value\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"preprocessed\"].apply(get_sentence_stress).values.reshape(-1, 1)\n",
    "\n",
    "class Sentiment(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"sentiment_score\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"preprocessed\"].apply(sentiment_polarity).values.reshape(-1, 1)\n",
    "\n",
    "class Redundance(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"redundance\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"preprocessed\"].apply(redundance).values.reshape(-1, 1)\n",
    "\n",
    "class UnusualWord(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"unusual_words\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"preprocessed\"].apply(word_choice).values.reshape(-1, 1)\n",
    "\n",
    "class Coherence(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"coherence\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"preprocessed\"].apply(coherence).values.reshape(-1, 1)\n",
    "\n",
    "class ReadingEase(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"reading_ease\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"text\"].apply(reading_ease).values.reshape(-1, 1)\n",
    "\n",
    "class GunningFog(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [\"gunning_fog\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[\"text\"].apply(gunning_fog).values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scaler = FunctionTransformer(lambda x: np.log1p(x), validate=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"input_handler\", InputHandler()),\n",
    "    (\"union\", FeatureUnion([\n",
    "        (\"preprocessed_features\", Pipeline([\n",
    "            (\"preprocessor\", TextPreprocessor()),\n",
    "            (\"features\", FeatureUnion([\n",
    "                (\"cons_density\", ConsDensity()),\n",
    "                (\"stress_value\", Pipeline([\n",
    "                    (\"extract\", Stress()),\n",
    "                    (\"scaler\", MinMaxScaler())\n",
    "                ])),\n",
    "                (\"sentiment_score\", Sentiment()),\n",
    "                (\"redundance\", Pipeline([\n",
    "                    (\"extract\", Redundance()),\n",
    "                    (\"log_scaling\", log_scaler)\n",
    "                ])),\n",
    "                (\"unusualword\", Pipeline([\n",
    "                    (\"extract\", UnusualWord()),\n",
    "                    (\"log_scaling\", log_scaler)\n",
    "                ])),\n",
    "                (\"coherence\", Coherence())\n",
    "            ]))\n",
    "        ])),\n",
    "        (\"original_text_features\", Pipeline([\n",
    "            (\"features\", FeatureUnion([\n",
    "                (\"wordcount\", Pipeline([\n",
    "                    (\"extract\", HowManyWords()),\n",
    "                    (\"scaler\", MinMaxScaler())\n",
    "                ])),\n",
    "                (\"readingease\", Pipeline([\n",
    "                    (\"extract\", ReadingEase()),\n",
    "                    (\"scaler\", MinMaxScaler())\n",
    "                ])),\n",
    "                (\"gunningfog\", Pipeline([\n",
    "                    (\"extract\", GunningFog()),\n",
    "                    (\"scaler\", MinMaxScaler())\n",
    "                ]))\n",
    "            ]))\n",
    "        ]))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "\n",
    "feature_names = [\n",
    "    \"cons_density\", \"stress_value\", \"sentiment_score\",\n",
    "    \"redundance\", \"unusual_words\", \"coherence\",\n",
    "    \"word_count\", \"reading_ease\", \"gunning_fog\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pipeline.fit_transform(X)\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With parallelism (*not working at the moment*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InputHandler(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         if isinstance(X, str):\n",
    "#             X = [X]\n",
    "#         return pd.DataFrame({\"text\": X})\n",
    "\n",
    "# class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"preprocessed\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         if isinstance(X, pd.DataFrame):\n",
    "#             X = X[\"text\"]\n",
    "#         cleaned = X.apply(basic_cleaning)\n",
    "#         return pd.DataFrame({\"preprocessed\": cleaned})\n",
    "\n",
    "# class ConsDensity(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"cons_density\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"preprocessed\"].apply(cons_density).values.reshape(-1, 1)\n",
    "\n",
    "# class Stress(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"stress_value\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"preprocessed\"].apply(get_sentence_stress).values.reshape(-1, 1)\n",
    "\n",
    "# class Sentiment(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"sentiment_score\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"preprocessed\"].apply(sentiment_polarity).values.reshape(-1, 1)\n",
    "\n",
    "# class Redundance(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"redundance\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"preprocessed\"].apply(redundance).values.reshape(-1, 1)\n",
    "\n",
    "# class UnusualWord(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"unusual_words\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"preprocessed\"].apply(word_choice).values.reshape(-1, 1)\n",
    "\n",
    "# class Coherence(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"coherence\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"preprocessed\"].apply(coherence).values.reshape(-1, 1)\n",
    "\n",
    "# class ReadingEase(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"reading_ease\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"text\"].apply(reading_ease).values.reshape(-1, 1)\n",
    "\n",
    "# class GunningFog(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         return [\"gunning_fog\"]\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[\"text\"].apply(gunning_fog).values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     (\"input_handler\", InputHandler()),\n",
    "#     (\"union\", FeatureUnion([\n",
    "#          (\"preprocessed_features\", Pipeline([\n",
    "#             (\"preprocessor\", TextPreprocessor()),\n",
    "#             (\"features\", FeatureUnion([\n",
    "#                 (\"cons_density\", ConsDensity()),\n",
    "#                 (\"stress_value\", Stress()),\n",
    "#                 (\"sentiment_score\", Sentiment()),\n",
    "#                 (\"redundance\", Redundance()),\n",
    "#                 (\"unusualword\", UnusualWord()),\n",
    "#                 (\"coherence\", Coherence())\n",
    "#             ]))\n",
    "#         ])),\n",
    "#         (\"original_text_features\", FeatureUnion([\n",
    "#             (\"readingease\", ReadingEase()),\n",
    "#             (\"gunningfog\", GunningFog())\n",
    "#         ]))\n",
    "#     ], n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "\n",
    "# feature_names = [\n",
    "#     \"cons_density\", \"stress_value\", \"sentiment_score\",\n",
    "#     \"redundance\", \"unusual_words\", \"coherence\",\n",
    "#     \"reading_ease\", \"gunning_fog\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_processed = pipeline.fit_transform(X)\n",
    "# X_processed_df = pd.DataFrame(X_processed, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_processed_df = pd.DataFrame(X_processed)\n",
    "# X_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_final = pd.concat([X, X_processed_df], axis=1)\n",
    "# X_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_processed_df,y,train_size=0.7, random_state= 1, stratify= y)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(50, activation = \"relu\", input_dim = X_processed_df.shape[1]))\n",
    "    model.add(layers.Dense(50, activation = \"relu\"))\n",
    "    model.add(layers.Dense(30, activation = \"relu\"))\n",
    "    model.add(layers.Dense(10, activation = \"relu\"))\n",
    "    model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "\n",
    "    return  model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience = 20,restore_best_weights=True, monitor='val_loss')\n",
    "compile_model(model)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=500,\n",
    "    callbacks=[es],\n",
    "    validation_split = 0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = baseline[1]\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import TFSMLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(filepath=r\"home\\romaric\\code\\nghia95\\fake-data-detector\\notebooks\\roma_models\\baseline_model.keras\")\n",
    "# model = load_model(r'/home/romaric/code/nghia95/fake-data-detector/notebooks/roma_models/baseline_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"Romaric is super awesome!\"\n",
    "\n",
    "X_processed = pipeline.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-data-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
